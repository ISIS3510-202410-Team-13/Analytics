# Analytics Pipeline

![Frame 71](https://github.com/ISIS3510-202410-Team-13/Analytics/assets/68788933/1aa76bc1-12e5-40ed-8cac-b1e0252c6e89)

The Analytics Pipeline for UniSchedule encompasses dashboards aimed at providing insightful reports with visualizations to answer critical business questions. These dashboards serve as a tool for gaining valuable insights into user behavior, app usage patterns, and other relevant metrics. The pipeline is designed to enable informed decision-making and improve overall user experience within the UniSchedule ecosystem.

## Approach

Our approach for the analytics pipeline focuses on creating reports with visualizations that address specific business questions. While the current implementation uses mock data, the ultimate goal is to connect the UniSchedule application with the pipeline to analyze real-time gathered data. This proof of concept demonstrates the potential of leveraging data analytics to enhance decision-making and improve user experience within the UniSchedule ecosystem. The mock data created for this was carefully examined to ensure that every source of information will be available from the architecture designed for the application. We're defining how the data will flow once the application is ready to be connected with the pipeline.

## Dashboards

The `dashboards` directory contains PDF files exported from the reports generated by the analytics pipeline. These reports include visualizations and insights derived from the processed data.

## Excel Mockups

The `excel_mockups` directory houses the data used to create each report. These mockups simulate the sources of data that will be processed through the ETL (Extract, Transform, Load) process in the future. The mock data represents various aspects of user interactions, app usage patterns, and other relevant metrics.

## Generators

The `generators` directory contains Python scripts responsible for creating the mock data used in the Excel mockups. These scripts generate synthetic data to simulate real-world scenarios and enable the creation of comprehensive reports.

## Cloud Functions

The `cloud_functions` directory contains Node.js scripts used to perform the ETL process. They are deployed using the Cloud Functions service in GCP. They work by querying the relevant raw data from No-SQL Databases stored in Firestore and updating the processed data to BigQuery tables which are then consumed by Looker Studio.


## Access to Live Reports

The following links provide acces to the reports developed in `looker-studio` (service provided by Google):

* BQ 2.1 - [Evaluate User Satisfaction Score for space booking feature](https://lookerstudio.google.com/reporting/1af52a7c-94e9-4970-ad3f-76ac91c16c24)
* BQ 2.3 - [Integration of social features, user retention, daily active usage](https://lookerstudio.google.com/reporting/9281694c-2631-46f5-8aab-0b23cb568ee9)
* BQ 3.4 - [Schedule customization underutilized features](https://lookerstudio.google.com/reporting/d9c878b6-38cf-4186-ac6f-20c4ac3192ce)
* BQ 4.1 - [Average weekly usage time by section for advertisement placement](https://lookerstudio.google.com/reporting/fb3c5144-378b-4239-a3df-c5327648dbe9)
* BQ 5.2 - [Preferred times and locations for meetings](https://lookerstudio.google.com/reporting/cfd155a3-f15c-40dc-8a40-ac8b716ae167)
